{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4497024,"sourceType":"datasetVersion","datasetId":1855453}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nos.listdir('../input/chess-pieces-detection-images-dataset')\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-27T22:17:02.735507Z","iopub.execute_input":"2023-11-27T22:17:02.735882Z","iopub.status.idle":"2023-11-27T22:17:02.745360Z","shell.execute_reply.started":"2023-11-27T22:17:02.735850Z","shell.execute_reply":"2023-11-27T22:17:02.743775Z"},"trusted":true},"execution_count":176,"outputs":[{"execution_count":176,"output_type":"execute_result","data":{"text/plain":"['Rook-resize',\n 'pawn_resized',\n 'knight-resize',\n 'Queen-Resized',\n 'bishop_resized']"},"metadata":{}}]},{"cell_type":"code","source":"Rooks = '../input/chess-pieces-detection-images-dataset/Rook-resize'\nPawns = '../input/chess-pieces-detection-images-dataset/pawn_resized'\nKnights = '../input/chess-pieces-detection-images-dataset/knight-resize'\nQueens = '../input/chess-pieces-detection-images-dataset/Queen-Resized'\nBishops = '../input/chess-pieces-detection-images-dataset/bishop_resized'\nclasses = [Rooks, Pawns, Knights, Queens, Bishops]","metadata":{"execution":{"iopub.status.busy":"2023-11-27T22:17:02.746802Z","iopub.execute_input":"2023-11-27T22:17:02.747607Z","iopub.status.idle":"2023-11-27T22:17:02.762769Z","shell.execute_reply.started":"2023-11-27T22:17:02.747569Z","shell.execute_reply":"2023-11-27T22:17:02.760992Z"},"trusted":true},"execution_count":177,"outputs":[]},{"cell_type":"code","source":"import cv2\n\"\"\"\ndef feature_builder(clss):\n    lst = []\n    for img in os.listdir(clss):\n        f = cv2.imread(os.path.join(clss,img))\n        f = cv2.cvtColor(f , cv2.COLOR_BGR2GRAY)        #convert colored images into grayscale format\n        f = cv2.resize(f , (50,50))\n        f = f / 255.0 #potential depricatipn candidate\n        f = np.expand_dims(f, axis=-1)\n        lst.append(f)\n    return lst\"\"\"\ndef feature_builder(clss):\n    lst = []\n    for img in os.listdir(clss):\n        f = cv2.imread(os.path.join(clss, img), cv2.IMREAD_GRAYSCALE)  # Read image in grayscale directly\n        f = cv2.resize(f, (100, 100))\n        f = f / 255.0  # Normalize the image data to 0-1\n        f = np.expand_dims(f, axis=-1)  # Add channel dimension\n        lst.append(f)\n    return lst","metadata":{"execution":{"iopub.status.busy":"2023-11-27T22:17:02.765123Z","iopub.execute_input":"2023-11-27T22:17:02.765647Z","iopub.status.idle":"2023-11-27T22:17:02.777763Z","shell.execute_reply.started":"2023-11-27T22:17:02.765617Z","shell.execute_reply":"2023-11-27T22:17:02.776565Z"},"trusted":true},"execution_count":178,"outputs":[]},{"cell_type":"code","source":"features = []\nfor c in classes:\n    features += feature_builder(c)\n\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-27T22:17:02.779036Z","iopub.execute_input":"2023-11-27T22:17:02.779372Z","iopub.status.idle":"2023-11-27T22:17:04.161366Z","shell.execute_reply.started":"2023-11-27T22:17:02.779344Z","shell.execute_reply":"2023-11-27T22:17:04.160547Z"},"trusted":true},"execution_count":179,"outputs":[]},{"cell_type":"code","source":"X = np.array(features)#convert to compatable numpy array\nX = X.reshape(-1, 100, 100, 1)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T22:17:04.163625Z","iopub.execute_input":"2023-11-27T22:17:04.164218Z","iopub.status.idle":"2023-11-27T22:17:04.181633Z","shell.execute_reply.started":"2023-11-27T22:17:04.164184Z","shell.execute_reply":"2023-11-27T22:17:04.180869Z"},"trusted":true},"execution_count":180,"outputs":[]},{"cell_type":"code","source":"#Label extraction\nlabels = []\ncount = 0\nfor c in classes:\n    for img in os.listdir(c):\n        labels.append(count)\n    count += 1\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-27T22:17:04.183499Z","iopub.execute_input":"2023-11-27T22:17:04.184110Z","iopub.status.idle":"2023-11-27T22:17:04.193306Z","shell.execute_reply.started":"2023-11-27T22:17:04.184078Z","shell.execute_reply":"2023-11-27T22:17:04.192200Z"},"trusted":true},"execution_count":181,"outputs":[]},{"cell_type":"code","source":"Y = np.array(labels)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T22:17:04.195275Z","iopub.execute_input":"2023-11-27T22:17:04.195785Z","iopub.status.idle":"2023-11-27T22:17:04.201995Z","shell.execute_reply.started":"2023-11-27T22:17:04.195752Z","shell.execute_reply":"2023-11-27T22:17:04.200738Z"},"trusted":true},"execution_count":182,"outputs":[]},{"cell_type":"code","source":"#Process x and y lists\nimport tensorflow.keras as keras\nfrom tensorflow.keras.utils import to_categorical, normalize\n\n#X = X.reshape(651,2500) #flatten the input for the model\n#X = normalize(X) #make sure that x has values between 0 and 1\nY = to_categorical(Y)\nprint(Y)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T22:17:04.203335Z","iopub.execute_input":"2023-11-27T22:17:04.203691Z","iopub.status.idle":"2023-11-27T22:17:04.216533Z","shell.execute_reply.started":"2023-11-27T22:17:04.203659Z","shell.execute_reply":"2023-11-27T22:17:04.215649Z"},"trusted":true},"execution_count":183,"outputs":[{"name":"stdout","text":"[[1. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n ...\n [0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 1.]]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#train test split\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,train_size=0.8)\nX_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-27T22:17:04.217619Z","iopub.execute_input":"2023-11-27T22:17:04.219185Z","iopub.status.idle":"2023-11-27T22:17:04.244358Z","shell.execute_reply.started":"2023-11-27T22:17:04.219104Z","shell.execute_reply":"2023-11-27T22:17:04.243141Z"},"trusted":true},"execution_count":184,"outputs":[{"execution_count":184,"output_type":"execute_result","data":{"text/plain":"(520, 100, 100, 1)"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 1)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(5))","metadata":{"execution":{"iopub.status.busy":"2023-11-27T22:17:04.246574Z","iopub.execute_input":"2023-11-27T22:17:04.247309Z","iopub.status.idle":"2023-11-27T22:17:04.334312Z","shell.execute_reply.started":"2023-11-27T22:17:04.247277Z","shell.execute_reply":"2023-11-27T22:17:04.333370Z"},"trusted":true},"execution_count":185,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory = model.fit(X_train, Y_train, epochs=10, \n                    validation_data=(X_test, Y_test))","metadata":{"execution":{"iopub.status.busy":"2023-11-27T22:17:04.335614Z","iopub.execute_input":"2023-11-27T22:17:04.335960Z","iopub.status.idle":"2023-11-27T22:17:35.565426Z","shell.execute_reply.started":"2023-11-27T22:17:04.335929Z","shell.execute_reply":"2023-11-27T22:17:35.563842Z"},"trusted":true},"execution_count":186,"outputs":[{"name":"stdout","text":"Epoch 1/10\n17/17 [==============================] - 4s 208ms/step - loss: 1.6958 - accuracy: 0.2135 - val_loss: 1.5965 - val_accuracy: 0.3053\nEpoch 2/10\n17/17 [==============================] - 3s 175ms/step - loss: 1.5549 - accuracy: 0.3077 - val_loss: 1.5335 - val_accuracy: 0.3435\nEpoch 3/10\n17/17 [==============================] - 3s 174ms/step - loss: 1.4358 - accuracy: 0.4000 - val_loss: 1.4290 - val_accuracy: 0.4275\nEpoch 4/10\n17/17 [==============================] - 3s 181ms/step - loss: 1.2215 - accuracy: 0.5269 - val_loss: 1.4796 - val_accuracy: 0.4122\nEpoch 5/10\n17/17 [==============================] - 3s 178ms/step - loss: 1.0019 - accuracy: 0.6019 - val_loss: 1.5079 - val_accuracy: 0.4580\nEpoch 6/10\n17/17 [==============================] - 3s 173ms/step - loss: 0.8055 - accuracy: 0.6942 - val_loss: 1.5303 - val_accuracy: 0.4580\nEpoch 7/10\n17/17 [==============================] - 3s 176ms/step - loss: 0.6608 - accuracy: 0.7577 - val_loss: 1.5708 - val_accuracy: 0.4962\nEpoch 8/10\n17/17 [==============================] - 3s 173ms/step - loss: 0.5222 - accuracy: 0.8115 - val_loss: 1.6546 - val_accuracy: 0.4733\nEpoch 9/10\n17/17 [==============================] - 3s 173ms/step - loss: 0.4047 - accuracy: 0.8673 - val_loss: 2.2394 - val_accuracy: 0.5725\nEpoch 10/10\n17/17 [==============================] - 3s 175ms/step - loss: 0.2968 - accuracy: 0.9135 - val_loss: 2.1041 - val_accuracy: 0.4885\n","output_type":"stream"}]}]}